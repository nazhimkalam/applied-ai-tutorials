# 6COSC00W - Applied AI Coursework 01 
**Project Title**: Lung Cancer Prognosis 

**Project Domain**: Hospital and medicine

## 1. Project Overview

This notebook will include both the code for the project implementations and a brief report about the project itself. The report will discuss the problem domain, compare AI techniques, choose an AI technique, provide information on the input dataset, and include a high-level project diagram. It will also compare the results of the evaluation and provide references for the LR research carried out in relation to the algorithms chosen.

## 2. Problem Domain

Hospitals and medicine was the chosen problem domain for this project. The selected problem under this domain was **Lung caner prognosis**, which in other words is referred to as lung cancer prediction. 

Roughly 10 million fatalities, or nearly one in six deaths, will be caused by cancer in 2020, making it the top cause of death globally. In a multi-stage process that typically progresses from a pre-cancerous lesion to a malignant tumor, cancer develops when normal cells undergo a transformation into tumor cells. *(World Health Organization, 2022)*

One form of cancer that starts in the lungs is lung cancer. Two spongy organs in your chest called your lungs allow you to breathe in oxygen and expel carbon dioxide. *(Mayo Clinic, 2021)*

## 3. AI Techniques Comparison

| No | Technique                               | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Strengths                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | Weakness                                                                                                                                                                                                                             | Advantages                                                                                                                                                                                                                                                              | Disadvantages                                                                                                                                                                                          | Implementation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Evalauation                                                 | Input Data Format                                                   | Output data Format                              |
|----|-----------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|---------------------------------------------------------------------|-------------------------------------------------|
| 1  | Decision Tree Classifier                | A **Decision tree** is a tree structure that resembles a flowchart where each leaf node symbolizes the result and each inside node indicates a characteristic (or attribute). The root node in a decision tree is the node that is located at the top. It gains the ability to divide data based on the value of an attribute. **Recursive partitioning**, as the name suggests, divides the tree in several ways. You can make decisions more easily with this flowchart-like framework. *(Safavian and Landgrebe, 1991)*                                                                                                                            | Simple to understand and to interpret, Requires little data preparation                                                                                                                                                                                                                                                                                                                                                                                                           | Decision trees are more likely to overfit the data since they can split on many different combination of features whereas in logistic regression we associate only one parameter with each feature                                   | Able to handle both numerical and categorical data, Able to handle both numerical and categorical data                                                                                                                                                                  | They are unstable, meaning that a small change in the data can lead to a large change in the structure of the optimal decision tree                                                                    | Given that we are dealing with text data (CSV format), we need to perform **data preprocessing and data visualization** in order to clean the data and handle imbalances etc… Once the cleaned data is ready to be used, we can make use of **sklearn** library to handle the algorithm imports **(DecisionTreeClassifier)** and perform the training, making sure the dataset is split into **train** and **test** along with the **dependent** and **independent** variables defined.                                                                                                                                                                                            | (Accuracy score: 93%, Sensitivity: 97%, Specificity: 96%)   | CSV input data format (attributes related to the CSV will be given) | 1 or 0, will be the output (1 = Yes and 0 = No) |
| 2  | SVM (Support Vector Machine) Classifier | **Support vector machines (SVMs)** are supervised machine learning models that apply classification algorithms to two-group classification issues. An SVM model can categorize new text after being given sets of labeled training data for each category. Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. *(Suykens and Vandewalle, 2004)*                                                                                                                                                                                                                           | SVM is more effective in high dimensional spaces.                                                                                                                                                                                                                                                                                                                                                                                                                                 | As the support vector classifier works by putting data points, above and below the classifying hyperplane there is no probabilistic explanation for the classification.                                                              | SVM works relatively well when there is a clear margin of separation between classes, SVM is relatively memory efficient                                                                                                                                                | SVM algorithm is not suitable for large data sets.                                                                                                                                                     | Given that we are dealing with text data (CSV format), we need to perform **data preprocessing and data visualization** in order to clean the data and handle imbalances etc… Once the cleaned data is ready to be used, we can make use of **sklearn** library to handle the algorithm imports **(SVC)** and perform the training, making sure the dataset is split into **train** and **test** along with the **dependent** and **independent** variables defined.                                                                                                                                                                                                               | (Accuracy score: 97.7%, Sensitivity: 97%, Specificity: 96%) | CSV input data format (attributes related to the CSV will be given) | 1 or 0, will be the output (1 = Yes and 0 = No) |
| 3  | KNN (K-Nearest Neighbor) Classifier     | The **K-Nearest Neighbors** algorithm, sometimes referred to as KNN or k-NN, is a supervised learning classifier that employs proximity to produce classifications or predictions about the grouping of a single data point. KNN finds the distances between a query and each example in the data, chooses the K instances closest to the query, and then, in the case of classification, votes for the label with the highest frequency or averages the labels (in the case of regression)*(Peterson, 2009)*                                                                                                                                         | No Training Period: KNN is called Lazy Learner (Instance based learning). It does not learn anything in the training period. It does not derive any discriminative function from the training data. In other words, there is no training period for it. It stores the training dataset and learns from it only at the time of making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g. SVM, Linear Regression etc. | Does not work well with high dimensions: The KNN algorithm doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate the distance in each dimension. | Since the KNN algorithm requires no training before making predictions, new data can be added seamlessly which will not impact the accuracy of the algorithm.                                                                                                           | Does not work well with large dataset: In large datasets, the cost of calculating the distance between the new point and each existing points is huge which degrades the performance of the algorithm. | Given that we are dealing with text data (CSV format), we need to perform **data preprocessing and data visualization** in order to clean the data and handle imbalances etc… Once the cleaned data is ready to be used, here we have to figure the best K value, we can achieve this using a Error Rate VS K Value Grraph, the goal is to find the K value with the Least Error Rate to get maximum accuracy. and then we can make use of **sklearn** library to handle the algorithm imports **KNeighborsClassifier** and perform the training, making sure the dataset is split into **train** and **test** along with the **dependent** and **independent** variables defined. | (Accuracy score: 94%, Sensitivity: 95%, Specificity: 94%)   | CSV input data format (attributes related to the CSV will be given) | 1 or 0, will be the output (1 = Yes and 0 = No) |
| 4  | Naive Bayes Classifier                  | A group of classification algorithms built on the Bayes' Theorem are known as **Naive Bayes Classifiers**. It is a family of algorithms rather than a single method, and they are all based on the idea that every pair of characteristics being categorized is independent of the other. The Naive Bayes Classifier is one of the most straightforward and efficient classification algorithms available today. It aids in the development of quick machine learning models capable of making accurate predictions. Being a probabilistic classifier, it makes predictions based on the likelihood that an object will occur.*(Mensah et al., 2021)* | It works well with high dimensions such as text classification.                                                                                                                                                                                                                                                                                                                                                                                                                   | If the independent assumption does not hold then performance is very low.                                                                                                                                                            | When the independent assumption holds then this classifier gives outstanding accuracy.                                                                                                                                                                                  | Smoothing turns out to be a over-head and a must to do step when probability of a feature turns out to be zero in a class.                                                                             | Given that we are dealing with text data (CSV format), we need to perform **data preprocessing and data visualization** in order to clean the data and handle imbalances etc… Once the cleaned data is ready to be used, we can make use of **sklearn** library to handle the algorithm imports **(GaussianNB)** and perform the training, making sure the dataset is split into **train** and **test** along with the **dependent** and **independent** variables defined.                                                                                                                                                                                                        | (Accuracy score: 95.0%, Sensitivity: 97%, Specificity: 96%) | CSV input data format (attributes related to the CSV will be given) | 1 or 0, will be the output (1 = Yes and 0 = No) |
| 5  | Neural Network (ANN Perceptron)         | A **Neural Network** is a collection of algorithms that aims to identify underlying relationships in a set of data using a method that imitates how the human brain functions. In this context, neural networks are systems of neurons that can be either organic or synthetic in origin. *(Hinton, Vinyals and Dean, 2015)* A **Perceptron** is a neural network unit that does certain computations to detect features or business intelligence in the input data. It is a function that maps its input “x,” which is multiplied by the learned weight coefficient, and generates an output value ”f(x). *(Rosenblatt, 2017)*                       | Neural networks are flexible and can be used for both regression and classification problems. Any data which can be made numeric can be used in the model, as neural network is a mathematical model with approximation functions.                                                                                                                                                                                                                                                | The very most disadvantage of a neural network is its black box nature. Because it has the ability to approximate any function, study its structure but don't give any insights on the structure of the function being approximated  | Neural networks are good to model with nonlinear data with large number of inputs; for example, images. It is reliable in an approach of tasks involving many features. It works by splitting the problem of classification into a layered network of simpler elements. | Neural network training requires lots of data                                                                                                                                                          | Given that we are dealing with text data (CSV format), we need to perform **data preprocessing and data visualization** in order to clean the data and handle imbalances etc… Once the cleaned data is ready to be used, we can make use of **sklearn** library to handle the algorithm imports **(MLPClassifier)** and perform the training, making sure the dataset is split into **train** and **test** along with the **dependent** and **independent** variables defined.                                                                                                                                                                                                     | (Accuracy: 86.0%, Sensitivity: 86.5%, Specificity: 85.0%)   | CSV input data format (attributes related to the CSV will be given) | 1 or 0, will be the output (1 = Yes and 0 = No) |
| 6  | Hybrid Model                            | A hybrid algorithm is an algorithm that combines two or more other algorithms that solve the same problem, and is mostly used in programming languages like C++, either choosing one (depending on the data), or switching between them over the course of the algorithm. Hybrid systems: A Hybrid system is an intelligent system that is framed by combining at least two intelligent technologies like Fuzzy Logic, Neural networks, Genetic algorithms, reinforcement learning, etc. (Chen and Chen, 2021)                                                                                                                                        | Higher performance, accuracy than filter                                                                                                                                                                                                                                                                                                                                                                                                                                          | Dependents of the combination of different fearture selection method                                                                                                                                                                 | Better computational complexity than wrapper                                                                                                                                                                                                                            | Classifier specific methods                                                                                                                                                                            | Given that we are dealing with text data (CSV format), we need to perform **data preprocessing and data visualization** in order to clean the data and handle imbalances etc… Once the cleaned data is ready to be used, then we get the best 2 algorithms which outperformed the rest to combine them using **(VotingClassifier)** from **sklearn** to make a combined model and perform the training, making sure the dataset is split into **train** and **test** along with the **dependent** and **independent** variables defined.                                                                                                                                           | (Accuracy: 97.0%, Sensitivity: 100.0%, Specificity: 94.0%)  | CSV input data format (attributes related to the CSV will be given) | 1 or 0, will be the output (1 = Yes and 0 = No) |

## 4. Choice of AI Technique

### 4.1 Input Data

### 4.2 High Level Diagram

## 5. Evaluation Results Comparison

## 6. References

World Health Organization (2022). Cancer. [online] World Health Organization. Available at: https://www.who.int/news-room/fact-sheets/detail/cancer.

‌Mayo Clinic (2021). Lung cancer - Symptoms and causes. [online] Mayo Clinic. Available at: https://www.mayoclinic.org/diseases-conditions/lung-cancer/symptoms-causes/syc-20374620.

Safavian, S.R. and Landgrebe, D. (1991). A survey of decision tree classifier methodology. IEEE Transactions on Systems, Man, and Cybernetics, 21(3), pp.660–674. doi:10.1109/21.97458.

‌Suykens, J. and Vandewalle, J. (2004). Least Squares Support Vector Machine Classifiers. undefined. [online] Available at: https://www.semanticscholar.org/paper/Least-Squares-Support-Vector-Machine-Classifiers-Suykens-Vandewalle/ccce1cf96f641b3581fba6f4ce2545f4135a15e3 [Accessed 16 Nov. 2022].

‌Peterson, L.E. (2009). K-nearest neighbor. undefined. [online] Available at: https://www.semanticscholar.org/paper/K-nearest-neighbor-Peterson/6e0121548ae114b8ed70b5189cbc4800d8b4290d [Accessed 16 Nov. 2022].

‌Mensah, I., Amoako-Yirenkyi, P., Frempong, N.K. and Lamptey, G.P. (2021). Wavelets Based Feature Extraction With PCA For Predicting Autism In Neonates Using Navïe Bayes Classifier. [online] www.semanticscholar.org. Available at: https://www.semanticscholar.org/paper/Wavelets-Based-Feature-Extraction-With-PCA-For-In-Mensah-Amoako-Yirenkyi/4b35141f735c9b20d2b89526f90351b1a4b138b5 [Accessed 16 Nov. 2022].

‌Hinton, G.E., Vinyals, O. and Dean, J. (2015). Distilling the Knowledge in a Neural Network. undefined. [online] Available at: https://www.semanticscholar.org/paper/Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals/0c908739fbff75f03469d13d4a1a07de3414ee19 [Accessed 16 Nov. 2022].

‌Rosenblatt, F.F. (2017). The perceptron: a probabilistic model for information storage and organization in the brain. [online] undefined. Available at: https://www.semanticscholar.org/paper/The-perceptron%3A-a-probabilistic-model-for-storage-Rosenblatt/5d11aad09f65431b5d3cb1d85328743c9e53ba96.

‌Chen, X. and Chen, W. (2021). GIS-based landslide susceptibility assessment using optimized hybrid machine learning methods. undefined. [online] Available at: https://www.semanticscholar.org/paper/GIS-based-landslide-susceptibility-assessment-using-Chen-Chen/c3991d3523e9ed77cea669940e824955b27179fa [Accessed 16 Nov. 2022].

‌

‌